import os
import json
from typing import Dict, List, Any
import asyncio
import re
from datetime import datetime

from app.agents.base import BaseAgent
from app.config.config import config
from app.tools.moderation import check_content_policy, anonymize_text

class EthicalFilterAgent(BaseAgent):
    """
    Agent responsible for reviewing OSINT reports for ethical concerns.
    Checks for privacy issues, graphic content, bias, and sensitive information.
    """
    
    def __init__(self):
        """Initialize the Ethical Filter Agent with appropriate tools."""
        super().__init__("EthicalFilter", config.ETHICAL_FILTER_PROMPT_TEMPLATE)
        
        # Register tools
        self.register_tool("check_content_policy", check_content_policy)
        self.register_tool("anonymize_text", anonymize_text)
    
    async def filter(self, draft_report: str) -> str:
        """
        Review and filter a draft OSINT report for ethical concerns.
        
        Args:
            draft_report: The draft report from the Reporter Agent
            
        Returns:
            The filtered and approved report
        """
        # Generate an ethical filter prompt
        prompt = self.format_prompt(draft_report=draft_report)
        
        # Check for content policy violations
        policy_check = await self.call_tool(
            "check_content_policy",
            text=draft_report
        )
        
        issues = []
        filtered_report = draft_report
        
        # Handle content policy issues
        if policy_check.get("violations", []):
            issues.extend(policy_check["violations"])
            
            # Anonymize personally identifiable information if needed
            if "pii" in policy_check.get("categories", []):
                filtered_report = await self.call_tool(
                    "anonymize_text",
                    text=filtered_report
                )
            
            # Add content warnings if needed
            if "graphic_content" in policy_check.get("categories", []):
                content_warning = """
## ⚠️ CONTENT WARNING ⚠️

This report contains descriptions of potentially disturbing events including violence and harm to civilians. Reader discretion is advised.

---

"""
                filtered_report = content_warning + filtered_report
        
        # Call LLM to review for bias and make ethical adjustments
        ethics_messages = [
            {"role": "system", "content": """You are an Ethical Filter Agent in an OSINT system. Review the draft report for:
            
            1. Biased or inflammatory language - replace with neutral, objective wording
            2. Unsubstantiated claims - ensure all assertions are properly sourced or qualified
            3. Privacy concerns - redact any remaining personal details of victims or witnesses
            4. Sensitive security information - remove details that could endanger individuals
            5. Balanced presentation - ensure the report doesn't unfairly favor one perspective
            
            Make minimal edits to preserve the substance while addressing ethical concerns.
            """},
            {"role": "user", "content": f"Draft Report:\n\n{filtered_report}\n\nIssues Already Identified: {', '.join(issues) if issues else 'None'}\n\nPlease review and provide the ethically-filtered version:"}
        ]
        
        ethical_review = await self.call_llm(ethics_messages)
        
        # Parse the response to extract the filtered report
        # Look for the report content after any explanatory text
        if "```" in ethical_review and ethical_review.count("```") >= 2:
            # Extract content between markdown code blocks
            filtered_report = re.search(r'```(?:markdown)?(.*?)```', ethical_review, re.DOTALL)
            if filtered_report:
                filtered_report = filtered_report.group(1).strip()
        else:
            # If no code blocks, try to remove any explanatory prefix
            if ethical_review.startswith("Here") and "\n\n" in ethical_review:
                filtered_report = ethical_review.split("\n\n", 1)[1]
            else:
                filtered_report = ethical_review
        
        # Add a disclaimer footer
        disclaimer = """

---

*This report was generated by an OSINT analysis system using verified open-source data. While efforts have been made to ensure accuracy, this report should be considered as preliminary research and not a definitive legal or historical account.*

"""
        if disclaimer not in filtered_report:
            filtered_report += disclaimer
        
        # Add the filtered report to memory
        self.add_to_memory({
            "original_length": len(draft_report),
            "filtered_length": len(filtered_report),
            "issues": issues,
            "timestamp": datetime.now().isoformat()
        })
        
        return filtered_report 